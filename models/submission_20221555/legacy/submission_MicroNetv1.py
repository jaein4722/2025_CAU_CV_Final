import torch
import torch.nn as nn
import torch.nn.functional as F

# --- MiniNetv2 í•µì‹¬ ëª¨ë“ˆë“¤ (ê²½ëŸ‰í™” ë²„ì „) ---

class SeparableConv2d(nn.Module):
    """Depthwise separable convolution - MiniNetv2ì˜ í•µì‹¬"""
    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, bias=True):
        super().__init__()
        self.depthwise = nn.Conv2d(in_channels, in_channels, kernel_size, stride, padding, dilation, groups=in_channels, bias=False)
        self.pointwise = nn.Conv2d(in_channels, out_channels, kernel_size=1, bias=bias)
        self.bn = nn.BatchNorm2d(in_channels, eps=1e-3)

    def forward(self, x):
        out = self.depthwise(x)
        out = self.bn(out)
        out = F.relu(out)
        out = self.pointwise(out)
        return out

class MultiDilationSeparableConv2d(nn.Module):
    """Multi-dilation separable conv - MiniNetv2ì˜ ì„±ëŠ¥ í•µì‹¬"""
    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, bias=True):
        super().__init__()
        padding2 = padding + (dilation - 1) * (kernel_size - 1) // 2
        self.depthwise1 = nn.Conv2d(in_channels, in_channels, kernel_size, stride, padding,         1, groups=in_channels, bias=False)
        self.depthwise2 = nn.Conv2d(in_channels, in_channels, kernel_size, stride, padding2, dilation, groups=in_channels, bias=False)
        self.pointwise  = nn.Conv2d(in_channels, out_channels, kernel_size=1, bias=bias)
        self.bn1 = nn.BatchNorm2d(in_channels, eps=1e-3)
        self.bn2 = nn.BatchNorm2d(in_channels, eps=1e-3)

    def forward(self, x):
        x1 = self.depthwise1(x)
        x1 = self.bn1(x1)
        x1 = F.relu(x1)

        x2 = self.depthwise2(x)
        x2 = self.bn2(x2)
        x2 = F.relu(x2)

        out = x1 + x2
        out = self.pointwise(out)
        return out

class MicroDownsampleModule(nn.Module):
    """ê²½ëŸ‰í™”ëœ ë‹¤ìš´ìƒ˜í”Œë§ ëª¨ë“ˆ"""
    def __init__(self, in_channels, out_channels):
        super().__init__()
        self.use_maxpool = in_channels < out_channels

        if not self.use_maxpool:
            channels_conv = out_channels
        else:
            channels_conv = out_channels - in_channels

        self.conv = nn.Conv2d(in_channels, channels_conv, kernel_size=3, stride=2, padding=1, dilation=1, bias=False)
        self.bn = nn.BatchNorm2d(out_channels, eps=1e-3)

    def forward(self, x):
        out = self.conv(x)

        if self.use_maxpool:
            x_pool = F.max_pool2d(x, kernel_size=2, stride=2)
            out = torch.cat([out, x_pool], dim=1)

        out = self.bn(out)
        return F.relu(out)

class MicroResidualConvModule(nn.Module):
    """ê²½ëŸ‰í™”ëœ Residual ëª¨ë“ˆ"""
    def __init__(self, channels, dilation, dropout=0):
        super().__init__()
        self.conv = SeparableConv2d(channels, channels, kernel_size=3, stride=1, padding=dilation, dilation=dilation, bias=False)
        self.bn = nn.BatchNorm2d(channels, eps=1e-3)
        self.dropout = nn.Dropout2d(dropout)

    def forward(self, x):
        out = self.conv(x)
        out = self.bn(out)
        out = self.dropout(out)
        return F.relu(x + out)

class MicroResidualMultiDilationConvModule(nn.Module):
    """ê²½ëŸ‰í™”ëœ Multi-dilation Residual ëª¨ë“ˆ"""
    def __init__(self, channels, dilation, dropout=0):
        super().__init__()
        self.conv = MultiDilationSeparableConv2d(channels, channels, kernel_size=3, stride=1, padding=1, dilation=dilation, bias=False)
        self.bn = nn.BatchNorm2d(channels, eps=1e-3)
        self.dropout = nn.Dropout2d(dropout)

    def forward(self, x):
        out = self.conv(x)
        out = self.bn(out)
        out = self.dropout(out)
        return F.relu(x + out)

class MicroUpsampleModule(nn.Module):
    """ê²½ëŸ‰í™”ëœ ì—…ìƒ˜í”Œë§ ëª¨ë“ˆ"""
    def __init__(self, in_channels, out_channels):
        super().__init__()
        self.conv = nn.ConvTranspose2d(in_channels, out_channels, kernel_size=3, stride=2, padding=1, output_padding=1, bias=False)
        self.bn = nn.BatchNorm2d(out_channels, eps=1e-3)

    def forward(self, x):
        out = self.conv(x)
        out = self.bn(out)
        return F.relu(out)

# --- MicroNetv1 ì¸ì½”ë” (ê·¹ë„ ê²½ëŸ‰í™”) ---

class MicroNetV1Encoder(nn.Module):
    """MicroNetv1 ì¸ì½”ë” - 518K â†’ 10K ë„ì „"""
    def __init__(self, in_channels):
        super().__init__()

        # ì±„ë„ ìˆ˜ ê·¹ë„ ê°ì†Œ: 3 â†’ 8 â†’ 16 â†’ 20 (vs ì›ë³¸ 3 â†’ 16 â†’ 64 â†’ 128)
        self.downsample_1 = MicroDownsampleModule(in_channels, 8)
        self.downsample_2 = MicroDownsampleModule(8, 16)
        
        # Downsample modules: 10ê°œ â†’ 1ê°œ
        self.downsample_modules = MicroResidualConvModule(16, 1, 0)
        
        self.downsample_3 = MicroDownsampleModule(16, 20)

        # Feature modules: 16ê°œ â†’ 2ê°œ, í•µì‹¬ dilation ratesë§Œ ìœ ì§€
        rates = [1, 2]  # vs ì›ë³¸ [1,2,1,4,1,8,1,16,1,1,1,2,1,4,1,8]
        self.feature_modules = nn.Sequential(*[MicroResidualMultiDilationConvModule(20, rate, 0.1) for rate in rates])

    def forward(self, x):
        d1 = self.downsample_1(x)
        d2 = self.downsample_2(d1)
        m1 = self.downsample_modules(d2)
        d3 = self.downsample_3(m1)
        m4 = self.feature_modules(d3)
        return m4

# --- ìµœì¢… ì œì¶œ ëª¨ë¸: MicroNetv1 (ê·¹ë„ ê²½ëŸ‰í™”) ---
class submission_MicroNetv1(nn.Module):
    """MicroNetv1 - MiniNetv2ì˜ 98% ê²½ëŸ‰í™” ë²„ì „"""
    def __init__(self, in_channels, num_classes, interpolate=True):
        super().__init__()

        self.interpolate = interpolate

        # ì¸ì½”ë” (ê·¹ë„ ê²½ëŸ‰í™”)
        self.encoder = MicroNetV1Encoder(in_channels)

        # Auxiliary path ì œê±° (íŒŒë¼ë¯¸í„° ì ˆì•½)

        # ì—…ìƒ˜í”Œ ë¸”ë¡ (ë‹¨ìˆœí™”)
        self.upsample_1 = MicroUpsampleModule(20, 16)
        
        # Upsample modules: 4ê°œ â†’ 1ê°œ
        self.upsample_mods = MicroResidualConvModule(16, 1, 0)

        # ì¶œë ¥ (ê²½ëŸ‰í™”)
        self.output_conv = nn.ConvTranspose2d(16, num_classes, kernel_size=3, stride=2, padding=1, output_padding=1, bias=True)

    def forward(self, x):
        # Auxiliary path ì œê±°ë¡œ ë‹¨ìˆœí™”
        enc = self.encoder(x)
        up1 = self.upsample_1(enc)
        m2 = self.upsample_mods(up1)

        out = self.output_conv(m2)

        if self.interpolate:
            out = F.interpolate(out, size=x.shape[2:], mode='bilinear', align_corners=True)

        return out

if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸ ë° íŒŒë¼ë¯¸í„° ê²€ì¦
    num_classes = 21
    net = submission_MicroNetv1(in_channels=3, num_classes=num_classes)
    p = sum(p.numel() for p in net.parameters() if p.requires_grad)
    print(f"Model: MicroNetv1 (MiniNetv2ì˜ 98% ê²½ëŸ‰í™”)")
    print(f"Trainable Params: {p:,} ({p/1e3:.2f} K)")
    
    # íŒŒë¼ë¯¸í„° ëª©í‘œ ê²€ì¦
    if p < 8000:
        print(f"âœ… ì´ìƒì  ë²”ìœ„: {p}/8,000 ({8000-p} ì—¬ìœ )")
    elif p < 10000:
        print(f"âœ… ëª©í‘œ ë‹¬ì„±: {p}/10,000 ({10000-p} ì—¬ìœ )")
    elif p <= 17000:
        print(f"âš ï¸  í—ˆìš© ë²”ìœ„ ë‚´: {p}/17,000 (hard cap)")
    else:
        print(f"âŒ íŒŒë¼ë¯¸í„° ì´ˆê³¼: {p}/17,000 ({p-17000} ì´ˆê³¼)")

    try:
        net.eval()  # í…ŒìŠ¤íŠ¸ ëª¨ë“œ
        x = torch.randn(1, 3, 256, 256)
        y = net(x)
        print(f"Input shape: {x.shape}")
        print(f"Output shape: {y.shape}")
        assert y.shape == (1, num_classes, 256, 256)
        print("âœ… ëª¨ë¸ ì‹¤í–‰ í…ŒìŠ¤íŠ¸ í†µê³¼")
        
        # ë‹¤ì–‘í•œ í´ë˜ìŠ¤ ìˆ˜ì— ëŒ€í•œ í…ŒìŠ¤íŠ¸
        for nc in [1, 2, 21]:
            net_test = submission_MicroNetv1(in_channels=3, num_classes=nc)
            net_test.eval()
            y_test = net_test(x)
            assert y_test.shape == (1, nc, 256, 256)
            print(f"âœ… {nc} í´ë˜ìŠ¤ í…ŒìŠ¤íŠ¸ í†µê³¼")
            
    except Exception as e:
        print(f"âŒ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        
    # MiniNetv2ì™€ ë¹„êµ ë¶„ì„
    print(f"\nğŸ“Š MiniNetv2 â†’ MicroNetv1 ë³€í™”:")
    print(f"  íŒŒë¼ë¯¸í„°: 518,227 â†’ {p:,} (-{518227-p:,}, -{(518227-p)/518227*100:.1f}%)")
    print(f"  ì±„ë„ ìˆ˜: 16â†’64â†’128 â†’ 8â†’16â†’20 (84.4% ê°ì†Œ)")
    print(f"  Feature modules: 16ê°œ â†’ 2ê°œ (87.5% ê°ì†Œ)")
    print(f"  Downsample modules: 10ê°œ â†’ 1ê°œ (90% ê°ì†Œ)")
    print(f"  Auxiliary path: ì œê±° (íŒŒë¼ë¯¸í„° ì ˆì•½)")
    print(f"  ëª©í‘œ: 0.4729 IoU ì„±ëŠ¥ ìµœëŒ€í•œ ìœ ì§€")
    
    # ëª¨ë“ˆë³„ íŒŒë¼ë¯¸í„° ë¶„ì„
    encoder_params = sum(p.numel() for p in net.encoder.parameters())
    upsample_params = sum(p.numel() for p in net.upsample_1.parameters()) + sum(p.numel() for p in net.upsample_mods.parameters())
    output_params = sum(p.numel() for p in net.output_conv.parameters())
    
    print(f"\nğŸ¯ ëª¨ë“ˆë³„ íŒŒë¼ë¯¸í„° ë¶„ë°°:")
    print(f"  Encoder: {encoder_params:,} ({encoder_params/p*100:.1f}%)")
    print(f"  Upsample: {upsample_params:,} ({upsample_params/p*100:.1f}%)")
    print(f"  Output: {output_params:,} ({output_params/p*100:.1f}%)")
    print(f"  íš¨ìœ¨ì„±: {p/518227:.4f} (ì›ë³¸ ëŒ€ë¹„ íŒŒë¼ë¯¸í„° ë¹„ìœ¨)")
    
    print(f"\nğŸš€ MicroNetv1 íŠ¹ì§•:")
    print(f"  âœ… Multi-dilation í•µì‹¬ ì•„ì´ë””ì–´ ë³´ì¡´")
    print(f"  âœ… Separable convolution í™œìš©")
    print(f"  âœ… Residual connections ìœ ì§€")
    print(f"  âœ… 98% íŒŒë¼ë¯¸í„° ê°ì†Œ ë‹¬ì„±")
    print(f"  ğŸ¯ ëª©í‘œ: MiniNetv2 ì„±ëŠ¥ì˜ 80-90% ìœ ì§€") 