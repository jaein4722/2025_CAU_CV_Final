import torch
import torch.nn as nn
import torch.nn.functional as F

# --- MicroNetv3: ì˜ë£Œ/íŠ¹ìˆ˜ ë„ë©”ì¸ íŠ¹í™” ëª¨ë“ˆë“¤ ---

class SeparableConv2d(nn.Module):
    """Depthwise separable convolution - ê¸°ë³¸ ëª¨ë“ˆ"""
    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, bias=True):
        super().__init__()
        self.depthwise = nn.Conv2d(in_channels, in_channels, kernel_size, stride, padding, dilation, groups=in_channels, bias=False)
        self.pointwise = nn.Conv2d(in_channels, out_channels, kernel_size=1, bias=bias)
        self.bn = nn.BatchNorm2d(in_channels, eps=1e-3)

    def forward(self, x):
        out = self.depthwise(x)
        out = self.bn(out)
        out = F.relu(out)
        out = self.pointwise(out)
        return out

class MultiDilationSeparableConv2d(nn.Module):
    """Multi-dilation separable conv - í•µì‹¬ ì„±ëŠ¥ ëª¨ë“ˆ"""
    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, bias=True):
        super().__init__()
        padding2 = padding + (dilation - 1) * (kernel_size - 1) // 2
        self.depthwise1 = nn.Conv2d(in_channels, in_channels, kernel_size, stride, padding,         1, groups=in_channels, bias=False)
        self.depthwise2 = nn.Conv2d(in_channels, in_channels, kernel_size, stride, padding2, dilation, groups=in_channels, bias=False)
        self.pointwise  = nn.Conv2d(in_channels, out_channels, kernel_size=1, bias=bias)
        self.bn1 = nn.BatchNorm2d(in_channels, eps=1e-3)
        self.bn2 = nn.BatchNorm2d(in_channels, eps=1e-3)

    def forward(self, x):
        x1 = self.depthwise1(x)
        x1 = self.bn1(x1)
        x1 = F.relu(x1)

        x2 = self.depthwise2(x)
        x2 = self.bn2(x2)
        x2 = F.relu(x2)

        out = x1 + x2
        out = self.pointwise(out)
        return out

class LightweightSpatialAttention(nn.Module):
    """ê²½ëŸ‰ ê³µê°„ ì–´í…ì…˜ - ì˜ë£Œ/íŠ¹ìˆ˜ ë„ë©”ì¸ ì„±ëŠ¥ í–¥ìƒ"""
    def __init__(self, channels, reduction=4):
        super().__init__()
        reduced_channels = max(channels // reduction, 1)  # ìµœì†Œ 1ì±„ë„ ë³´ì¥
        self.conv1 = nn.Conv2d(channels, reduced_channels, kernel_size=1, bias=False)
        self.conv2 = nn.Conv2d(reduced_channels, 1, kernel_size=1, bias=False)
        self.bn = nn.BatchNorm2d(reduced_channels, eps=1e-3)
        
    def forward(self, x):
        # Global context
        gap = F.adaptive_avg_pool2d(x, 1)
        att = self.conv1(gap)
        att = self.bn(att)
        att = F.relu(att)
        att = self.conv2(att)
        att = torch.sigmoid(att)
        
        # Spatial attention
        spatial = torch.mean(x, dim=1, keepdim=True)
        spatial = torch.sigmoid(spatial)
        
        # Combine attentions
        combined_att = att * spatial
        return x * combined_att

class MultiScalePyramidModule(nn.Module):
    """Multi-scale pyramid fusion - ë‹¤ì–‘í•œ ìŠ¤ì¼€ì¼ íŠ¹ì§• ìœµí•©"""
    def __init__(self, in_channels, out_channels):
        super().__init__()
        mid_channels = max(in_channels // 2, 4)  # ìµœì†Œ 4ì±„ë„ ë³´ì¥
        
        # Multi-scale branches (2ê°œë¡œ ì¶•ì†Œ)
        self.branch1 = SeparableConv2d(in_channels, mid_channels, 3, padding=1, dilation=1)
        self.branch2 = SeparableConv2d(in_channels, mid_channels, 3, padding=2, dilation=2)
        
        # Fusion
        self.fusion = nn.Conv2d(mid_channels * 2, out_channels, kernel_size=1, bias=False)
        self.bn = nn.BatchNorm2d(out_channels, eps=1e-3)
        
    def forward(self, x):
        b1 = self.branch1(x)
        b2 = self.branch2(x)
        
        fused = torch.cat([b1, b2], dim=1)
        out = self.fusion(fused)
        out = self.bn(out)
        return F.relu(out)

class EdgeEnhancementBranch(nn.Module):
    """ê²½ê³„ ê°•í™” ë¸Œëœì¹˜ - ì •í™•í•œ segmentation ê²½ê³„"""
    def __init__(self, in_channels, out_channels):
        super().__init__()
        mid_channels = max(in_channels // 4, 2)  # ë” ê²½ëŸ‰í™”
        self.edge_conv1 = nn.Conv2d(in_channels, mid_channels, kernel_size=3, padding=1, bias=False)
        self.edge_conv2 = nn.Conv2d(mid_channels, out_channels, kernel_size=1, bias=False)
        self.bn1 = nn.BatchNorm2d(mid_channels, eps=1e-3)
        self.bn2 = nn.BatchNorm2d(out_channels, eps=1e-3)
        
    def forward(self, x):
        # Edge detection
        edge = self.edge_conv1(x)
        edge = self.bn1(edge)
        edge = F.relu(edge)
        
        edge = self.edge_conv2(edge)
        edge = self.bn2(edge)
        
        return torch.sigmoid(edge)

class MicroDownsampleModule(nn.Module):
    """ë‹¤ìš´ìƒ˜í”Œë§ ëª¨ë“ˆ"""
    def __init__(self, in_channels, out_channels):
        super().__init__()
        self.use_maxpool = in_channels < out_channels

        if not self.use_maxpool:
            channels_conv = out_channels
        else:
            channels_conv = out_channels - in_channels

        self.conv = nn.Conv2d(in_channels, channels_conv, kernel_size=3, stride=2, padding=1, dilation=1, bias=False)
        self.bn = nn.BatchNorm2d(out_channels, eps=1e-3)

    def forward(self, x):
        out = self.conv(x)

        if self.use_maxpool:
            x_pool = F.max_pool2d(x, kernel_size=2, stride=2)
            out = torch.cat([out, x_pool], dim=1)

        out = self.bn(out)
        return F.relu(out)

class MicroResidualConvModule(nn.Module):
    """Residual ëª¨ë“ˆ with attention"""
    def __init__(self, channels, dilation, dropout=0, use_attention=False):
        super().__init__()
        self.conv = SeparableConv2d(channels, channels, kernel_size=3, stride=1, padding=dilation, dilation=dilation, bias=False)
        self.bn = nn.BatchNorm2d(channels, eps=1e-3)
        self.dropout = nn.Dropout2d(dropout)
        
        self.use_attention = use_attention
        if use_attention:
            self.attention = LightweightSpatialAttention(channels)

    def forward(self, x):
        out = self.conv(x)
        out = self.bn(out)
        out = self.dropout(out)
        
        if self.use_attention:
            out = self.attention(out)
            
        return F.relu(x + out)

class MicroResidualMultiDilationConvModule(nn.Module):
    """Multi-dilation Residual ëª¨ë“ˆ with attention"""
    def __init__(self, channels, dilation, dropout=0, use_attention=False):
        super().__init__()
        self.conv = MultiDilationSeparableConv2d(channels, channels, kernel_size=3, stride=1, padding=1, dilation=dilation, bias=False)
        self.bn = nn.BatchNorm2d(channels, eps=1e-3)
        self.dropout = nn.Dropout2d(dropout)
        
        self.use_attention = use_attention
        if use_attention:
            self.attention = LightweightSpatialAttention(channels)

    def forward(self, x):
        out = self.conv(x)
        out = self.bn(out)
        out = self.dropout(out)
        
        if self.use_attention:
            out = self.attention(out)
            
        return F.relu(x + out)

class MicroUpsampleModule(nn.Module):
    """ì—…ìƒ˜í”Œë§ ëª¨ë“ˆ"""
    def __init__(self, in_channels, out_channels):
        super().__init__()
        self.conv = nn.ConvTranspose2d(in_channels, out_channels, kernel_size=3, stride=2, padding=1, output_padding=1, bias=False)
        self.bn = nn.BatchNorm2d(out_channels, eps=1e-3)

    def forward(self, x):
        out = self.conv(x)
        out = self.bn(out)
        return F.relu(out)

# --- MicroNetv3 ì¸ì½”ë” (ì˜ë£Œ/íŠ¹ìˆ˜ ë„ë©”ì¸ íŠ¹í™”) ---

class MicroNetV3Encoder(nn.Module):
    """MicroNetv3 ì¸ì½”ë” - ì˜ë£Œ/íŠ¹ìˆ˜ ë„ë©”ì¸ ì„±ëŠ¥ í–¥ìƒ"""
    def __init__(self, in_channels):
        super().__init__()

        # ì±„ë„ ìˆ˜: 3 â†’ 10 â†’ 18 â†’ 24 (MicroNetv2ì™€ ë™ì¼)
        self.downsample_1 = MicroDownsampleModule(in_channels, 10)
        self.downsample_2 = MicroDownsampleModule(10, 18)
        
        # Downsample modules with attention (1ê°œë¡œ ì¶•ì†Œ)
        self.downsample_modules = nn.Sequential(*[
            MicroResidualConvModule(18, 1, 0, use_attention=True)
        ])
        
        self.downsample_3 = MicroDownsampleModule(18, 24)

        # Feature modules with attention (ì˜ë£Œ ë„ë©”ì¸ íŠ¹í™”) - ê²½ëŸ‰í™”
        rates = [1, 2]
        self.feature_modules = nn.Sequential(*[
            MicroResidualMultiDilationConvModule(24, rates[0], 0.1, use_attention=True),
            MicroResidualMultiDilationConvModule(24, rates[1], 0.1, use_attention=False)
        ])
        
        # Multi-scale pyramid for better feature representation
        self.pyramid = MultiScalePyramidModule(24, 24)

    def forward(self, x):
        d1 = self.downsample_1(x)
        d2 = self.downsample_2(d1)
        m2 = self.downsample_modules(d2)
        d3 = self.downsample_3(m2)
        m4 = self.feature_modules(d3)
        
        # Multi-scale enhancement
        m4_enhanced = self.pyramid(m4)
        
        return m4_enhanced, d2  # skip connection

# --- ìµœì¢… ì œì¶œ ëª¨ë¸: MicroNetv3 (ì˜ë£Œ/íŠ¹ìˆ˜ ë„ë©”ì¸ íŠ¹í™”) ---
class submission_MicroNetv3(nn.Module):
    """MicroNetv3 - ì˜ë£Œ/íŠ¹ìˆ˜ ë„ë©”ì¸ ì„±ëŠ¥ í–¥ìƒ"""
    def __init__(self, in_channels, num_classes, interpolate=True):
        super().__init__()

        self.interpolate = interpolate

        # ì¸ì½”ë” (ì˜ë£Œ ë„ë©”ì¸ íŠ¹í™”)
        self.encoder = MicroNetV3Encoder(in_channels)

        # Auxiliary path (ì €ìˆ˜ì¤€ íŠ¹ì§• ë³´ì¡´)
        self.aux_downsample = MicroDownsampleModule(in_channels, 10)
        self.aux_refine = MicroResidualConvModule(10, 1, 0, use_attention=True)

        # Edge enhancement branch (ê²½ê³„ ì •í™•ë„ í–¥ìƒ)
        self.edge_branch = EdgeEnhancementBranch(24, 1)

        # ì—…ìƒ˜í”Œ ë¸”ë¡ (ê°•í™”)
        self.upsample_1 = MicroUpsampleModule(24, 18)
        
        # Upsample modules with attention (1ê°œë¡œ ì¶•ì†Œ)
        self.upsample_mods = nn.Sequential(*[
            MicroResidualConvModule(18, 1, 0, use_attention=True)
        ])

        # ì¶œë ¥ (ê°œì„ )
        self.output_conv = nn.ConvTranspose2d(18, num_classes, kernel_size=3, stride=2, padding=1, output_padding=1, bias=True)

    def forward(self, x):
        # Auxiliary path (ì €ìˆ˜ì¤€ íŠ¹ì§• ë³´ì¡´)
        aux = self.aux_downsample(x)
        aux = self.aux_refine(aux)
        
        # Main encoder
        enc, skip = self.encoder(x)
        
        # Edge enhancement
        edge_map = self.edge_branch(enc)
        
        # Apply edge enhancement to features
        enc_enhanced = enc * (1 + edge_map)
        
        # Decoder with skip connection
        up1 = self.upsample_1(enc_enhanced)
        
        # Skip connection í™œìš©
        if up1.shape[2:] == skip.shape[2:]:
            up1 = up1 + skip
        
        # Auxiliary pathì™€ ê²°í•©
        if up1.shape[2:] == aux.shape[2:]:
            up1 = up1 + aux
            
        m3 = self.upsample_mods(up1)

        out = self.output_conv(m3)

        if self.interpolate:
            out = F.interpolate(out, size=x.shape[2:], mode='bilinear', align_corners=True)

        return out

if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸ ë° íŒŒë¼ë¯¸í„° ê²€ì¦
    num_classes = 21
    net = submission_MicroNetv3(in_channels=3, num_classes=num_classes)
    p = sum(p.numel() for p in net.parameters() if p.requires_grad)
    print(f"Model: MicroNetv3 (ì˜ë£Œ/íŠ¹ìˆ˜ ë„ë©”ì¸ íŠ¹í™”)")
    print(f"Trainable Params: {p:,} ({p/1e3:.2f} K)")
    
    # íŒŒë¼ë¯¸í„° ëª©í‘œ ê²€ì¦
    if p <= 17000:
        print(f"âœ… Hard cap ì¤€ìˆ˜: {p}/17,000 ({17000-p} ì—¬ìœ )")
    else:
        print(f"âŒ Hard cap ì´ˆê³¼: {p}/17,000 ({p-17000} ì´ˆê³¼)")
        
    if p < 10000:
        print(f"âœ… ì´ìƒì  ë²”ìœ„: {p}/10,000")
    elif p < 15000:
        print(f"âœ… ìš°ìˆ˜ ë²”ìœ„: {p}/15,000")

    try:
        net.eval()  # í…ŒìŠ¤íŠ¸ ëª¨ë“œ
        x = torch.randn(1, 3, 256, 256)
        y = net(x)
        print(f"Input shape: {x.shape}")
        print(f"Output shape: {y.shape}")
        assert y.shape == (1, num_classes, 256, 256)
        print("âœ… ëª¨ë¸ ì‹¤í–‰ í…ŒìŠ¤íŠ¸ í†µê³¼")
        
        # ë‹¤ì–‘í•œ í´ë˜ìŠ¤ ìˆ˜ì— ëŒ€í•œ í…ŒìŠ¤íŠ¸
        for nc in [1, 2, 21]:
            net_test = submission_MicroNetv3(in_channels=3, num_classes=nc)
            net_test.eval()
            y_test = net_test(x)
            assert y_test.shape == (1, nc, 256, 256)
            print(f"âœ… {nc} í´ë˜ìŠ¤ í…ŒìŠ¤íŠ¸ í†µê³¼")
            
    except Exception as e:
        print(f"âŒ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        
    # v2ì™€ ë¹„êµ ë¶„ì„
    print(f"\nğŸ“Š MicroNetv2 â†’ MicroNetv3 ê°œì„ ì‚¬í•­:")
    print(f"  íŒŒë¼ë¯¸í„°: 15,459 â†’ {p:,} ({p-15459:+,}, {(p-15459)/15459*100:+.1f}%)")
    print(f"  ìƒˆë¡œìš´ ëª¨ë“ˆ:")
    print(f"    âœ… LightweightSpatialAttention - ì¤‘ìš” ì˜ì—­ ì§‘ì¤‘")
    print(f"    âœ… MultiScalePyramidModule - ë‹¤ì–‘í•œ ìŠ¤ì¼€ì¼ ìœµí•©")
    print(f"    âœ… EdgeEnhancementBranch - ê²½ê³„ ì •í™•ë„ í–¥ìƒ")
    print(f"    âœ… Attention-enhanced modules - ì˜ë£Œ ë„ë©”ì¸ íŠ¹í™”")
    
    # ëª¨ë“ˆë³„ íŒŒë¼ë¯¸í„° ë¶„ì„
    encoder_params = sum(p.numel() for p in net.encoder.parameters())
    aux_params = sum(p.numel() for p in net.aux_downsample.parameters()) + sum(p.numel() for p in net.aux_refine.parameters())
    edge_params = sum(p.numel() for p in net.edge_branch.parameters())
    upsample_params = sum(p.numel() for p in net.upsample_1.parameters()) + sum(p.numel() for p in net.upsample_mods.parameters())
    output_params = sum(p.numel() for p in net.output_conv.parameters())
    
    print(f"\nğŸ¯ ëª¨ë“ˆë³„ íŒŒë¼ë¯¸í„° ë¶„ë°°:")
    print(f"  Encoder: {encoder_params:,} ({encoder_params/p*100:.1f}%)")
    print(f"  Auxiliary: {aux_params:,} ({aux_params/p*100:.1f}%)")
    print(f"  Edge Branch: {edge_params:,} ({edge_params/p*100:.1f}%)")
    print(f"  Upsample: {upsample_params:,} ({upsample_params/p*100:.1f}%)")
    print(f"  Output: {output_params:,} ({output_params/p*100:.1f}%)")
    
    print(f"\nğŸš€ MicroNetv3 íŠ¹ì§•:")
    print(f"  ğŸ¯ ì˜ë£Œ/íŠ¹ìˆ˜ ë„ë©”ì¸ íŠ¹í™” ì„¤ê³„")
    print(f"  âœ… Spatial Attention - ETIS, CFD, CarDD ì„±ëŠ¥ í–¥ìƒ")
    print(f"  âœ… Multi-Scale Pyramid - ë‹¤ì–‘í•œ ê°ì²´ í¬ê¸° ëŒ€ì‘")
    print(f"  âœ… Edge Enhancement - ì •í™•í•œ ê²½ê³„ ë¶„í• ")
    print(f"  âœ… Hard cap 17K ì¤€ìˆ˜")
    print(f"  ğŸ“ˆ ëª©í‘œ: ETIS 0.32â†’0.4+, CFD 0.34â†’0.4+, CarDD 0.35â†’0.4+")
    
    # ì„±ëŠ¥ ì˜ˆì¸¡
    print(f"\nğŸ“ˆ ì˜ˆìƒ ì„±ëŠ¥ í–¥ìƒ:")
    print(f"  ETIS (í´ë¦½): 0.3215 â†’ 0.40+ (ì‘ì€ ê°ì²´ íƒì§€ ê°œì„ )")
    print(f"  CFD (í¬ë™): 0.3366 â†’ 0.42+ (ì„ í˜• êµ¬ì¡° íƒì§€ ê°œì„ )")
    print(f"  CarDD (ì†ìƒ): 0.3454 â†’ 0.38+ (ë³µì¡í•œ íŒ¨í„´ ì¸ì‹ ê°œì„ )")
    print(f"  VOC: 0.1205 â†’ 0.13+ (ì¼ë°˜ ë„ë©”ì¸ ìœ ì§€)")
    print(f"  CVPPP: 0.9185 â†’ 0.92+ (ì´ë¯¸ ìš°ìˆ˜í•œ ì„±ëŠ¥ ìœ ì§€)")
    print(f"  Mean IoU: 0.4085 â†’ 0.43+ (ì „ì²´ì  ì„±ëŠ¥ í–¥ìƒ)") 