import torch
import torch.nn as nn
import torch.nn.functional as F
import math


class GhostModule(nn.Module):
    """Simplified Ghost Module with reduced parameters"""
    def __init__(self, inp, oup, kernel_size=1, ratio=2, dw_size=3, stride=1):
        super(GhostModule, self).__init__()
        self.oup = oup
        init_channels = math.ceil(oup / ratio)
        new_channels = init_channels * (ratio - 1)

        self.primary_conv = nn.Sequential(
            nn.Conv2d(inp, init_channels, kernel_size, stride, kernel_size//2, bias=False),
            nn.BatchNorm2d(init_channels),
            nn.SiLU(inplace=True),
        )

        self.cheap_operation = nn.Sequential(
            nn.Conv2d(init_channels, new_channels, dw_size, 1, dw_size//2, groups=init_channels, bias=False),
            nn.BatchNorm2d(new_channels),
            nn.SiLU(inplace=True),
        )

    def forward(self, x):
        x1 = self.primary_conv(x)
        x2 = self.cheap_operation(x1)
        out = torch.cat([x1, x2], dim=1)
        return out[:, :self.oup, :, :]


class SEBlock(nn.Module):
    """Ultra-lightweight SE block"""
    def __init__(self, channel, reduction=16):
        super(SEBlock, self).__init__()
        self.avg_pool = nn.AdaptiveAvgPool2d(1)
        reduced_ch = max(1, channel // reduction)
        self.squeeze = nn.Conv2d(channel, reduced_ch, 1)
        self.excitation = nn.Conv2d(reduced_ch, channel, 1)
        
    def forward(self, x):
        b, c, _, _ = x.size()
        y = self.avg_pool(x)
        y = F.relu(self.squeeze(y), inplace=True)
        y = torch.sigmoid(self.excitation(y))
        return x * y


class DepthwiseSeparableConv(nn.Module):
    """Depthwise separable convolution"""
    def __init__(self, inp, oup, stride=1):
        super(DepthwiseSeparableConv, self).__init__()
        self.depthwise = nn.Conv2d(inp, inp, 3, stride, 1, groups=inp, bias=False)
        self.pointwise = nn.Conv2d(inp, oup, 1, bias=False)
        self.bn1 = nn.BatchNorm2d(inp)
        self.bn2 = nn.BatchNorm2d(oup)
        
    def forward(self, x):
        x = F.silu(self.bn1(self.depthwise(x)), inplace=True)
        x = F.silu(self.bn2(self.pointwise(x)), inplace=True)
        return x


class submission_GhostUNet(nn.Module):
    """Ultra-lightweight Ghost U-Net for semantic segmentation
    
    Optimized for <10k parameters while maintaining performance
    """
    
    def __init__(self, in_channels, num_classes):
        super(submission_GhostUNet, self).__init__()
        
        # Encoder - very lightweight
        self.enc1 = nn.Sequential(
            nn.Conv2d(3, 8, 3, 1, 1, bias=False),
            nn.BatchNorm2d(8),
            nn.SiLU(inplace=True)
        )
        
        self.enc2 = DepthwiseSeparableConv(8, 12, stride=2)    # 256->128
        self.enc3 = DepthwiseSeparableConv(12, 16, stride=2)   # 128->64  
        self.enc4 = DepthwiseSeparableConv(16, 20, stride=2)   # 64->32
        
        # Bottleneck with attention
        self.bottleneck = nn.Sequential(
            GhostModule(20, 24, kernel_size=3),
            SEBlock(24, reduction=8)
        )
        
        # Decoder - simple and efficient
        self.up4 = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False)
        self.dec4 = nn.Sequential(
            nn.Conv2d(24 + 16, 16, 3, 1, 1, bias=False),  # 24+16->16
            nn.BatchNorm2d(16),
            nn.SiLU(inplace=True)
        )
        
        self.up3 = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False)
        self.dec3 = nn.Sequential(
            nn.Conv2d(16 + 12, 12, 3, 1, 1, bias=False),  # 16+12->12
            nn.BatchNorm2d(12),
            nn.SiLU(inplace=True)
        )
        
        self.up2 = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False)
        self.dec2 = nn.Sequential(
            nn.Conv2d(12 + 8, 8, 3, 1, 1, bias=False),   # 12+8->8
            nn.BatchNorm2d(8),
            nn.SiLU(inplace=True)
        )
        
        # Output
        self.output = nn.Conv2d(8, num_classes, 1)
        
        self._initialize_weights()
        
    def _initialize_weights(self):
        """Initialize weights with Kaiming normal"""
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')
                if m.bias is not None:
                    nn.init.constant_(m.bias, 0)
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.constant_(m.weight, 1)
                nn.init.constant_(m.bias, 0)
    
    def forward(self, x):
        # Encoder
        e1 = self.enc1(x)      # 8, 256x256
        e2 = self.enc2(e1)     # 12, 128x128
        e3 = self.enc3(e2)     # 16, 64x64
        e4 = self.enc4(e3)     # 20, 32x32
        
        # Bottleneck  
        b = self.bottleneck(e4)  # 24, 32x32
        
        # Decoder with skip connections
        # Note: Skip feature alignment
        d4 = self.up4(b)                           # 24, 64x64
        e3_aligned = F.interpolate(e3, size=d4.shape[2:], mode='bilinear', align_corners=False)
        d4 = torch.cat([d4, e3_aligned], dim=1)    # 24+16=40, 64x64
        d4 = self.dec4(d4)                         # 16, 64x64
        
        d3 = self.up3(d4)                          # 16, 128x128  
        e2_aligned = F.interpolate(e2, size=d3.shape[2:], mode='bilinear', align_corners=False)
        d3 = torch.cat([d3, e2_aligned], dim=1)    # 16+12=28, 128x128
        d3 = self.dec3(d3)                         # 12, 128x128
        
        d2 = self.up2(d3)                          # 12, 256x256
        e1_aligned = F.interpolate(e1, size=d2.shape[2:], mode='bilinear', align_corners=False)
        d2 = torch.cat([d2, e1_aligned], dim=1)    # 12+8=20, 256x256
        d2 = self.dec2(d2)                         # 8, 256x256
        
        # Output
        out = self.output(d2)                      # num_classes, 256x256
        
        return out


if __name__ == "__main__":
    # Test parameter count
    model_binary = submission_GhostUNet(3, 2)
    model_voc = submission_GhostUNet(3, 21)
    
    total_params_binary = sum(p.numel() for p in model_binary.parameters() if p.requires_grad)
    total_params_voc = sum(p.numel() for p in model_voc.parameters() if p.requires_grad)
    
    print(f"Binary model parameters: {total_params_binary:,}")
    print(f"VOC model parameters: {total_params_voc:,}")
    
    # Test forward pass
    x = torch.randn(1, 3, 256, 256)
    
    with torch.no_grad():
        out_binary = model_binary(x)
        out_voc = model_voc(x)
        
    print(f"Binary output shape: {out_binary.shape}")
    print(f"VOC output shape: {out_voc.shape}")
    
    # Verify shapes
    assert out_binary.shape == (1, 2, 256, 256), f"Binary output shape mismatch: {out_binary.shape}"
    assert out_voc.shape == (1, 21, 256, 256), f"VOC output shape mismatch: {out_voc.shape}"
    
    # Check parameter constraints
    print(f"\nParameter check:")
    print(f"Binary: {total_params_binary} {'âœ…' if total_params_binary <= 10000 else 'âŒ'}")
    print(f"VOC: {total_params_voc} {'âœ…' if total_params_voc <= 10000 else 'âŒ'}")
    
    if total_params_binary <= 10000 and total_params_voc <= 10000:
        print("\nðŸŽ‰ All constraints satisfied! Ready for training.")
    else:
        print("\nâš ï¸ Parameter count exceeds 10k limit. Need further optimization.") 