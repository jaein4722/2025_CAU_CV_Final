import torch
import torch.nn as nn
import torch.nn.functional as F

# --- Helper Modules (v5ì™€ ë™ì¼) ---

class DWSConv(nn.Module):
    """Depthwise Separable Convolution - íŒŒë¼ë¯¸í„° íš¨ìœ¨ì„±ì„ ìœ„í•œ í•µì‹¬ ëª¨ë“ˆ"""
    def __init__(self, nIn, nOut, kSize, stride=1, padding=0, dilation=(1, 1), bn_acti=True, bias=False):
        super().__init__()
        self.depthwise = nn.Conv2d(nIn, nIn, kernel_size=kSize, stride=stride, padding=padding,
                                   dilation=dilation, groups=nIn, bias=bias)
        self.pointwise = nn.Conv2d(nIn, nOut, kernel_size=1, stride=1, padding=0, bias=bias)
        self.bn_acti = bn_acti
        if self.bn_acti:
            self.bn = nn.BatchNorm2d(nOut, eps=1e-3)
            self.acti = nn.SiLU(inplace=True)

    def forward(self, x):
        x = self.depthwise(x)
        x = self.pointwise(x)
        if self.bn_acti:
            x = self.bn(x)
            x = self.acti(x)
        return x

class Conv(nn.Module):
    """1x1 Convë‚˜ Grouped Convë¥¼ ìœ„í•œ í‘œì¤€ ëª¨ë“ˆ"""
    def __init__(self, nIn, nOut, kSize, stride, padding, dilation=(1, 1), groups=1, bn_acti=False, bias=False):
        super().__init__()
        self.bn_acti = bn_acti
        self.conv = nn.Conv2d(nIn, nOut, kernel_size=kSize, stride=stride, padding=padding,
                              dilation=dilation, groups=groups, bias=bias)
        if self.bn_acti:
            self.bn = nn.BatchNorm2d(nOut, eps=1e-3)
            self.acti = nn.SiLU(inplace=True)

    def forward(self, input):
        output = self.conv(input)
        if self.bn_acti:
            output = self.bn(output)
            output = self.acti(output)
        return output

class DownSamplingBlock(nn.Module):
    """íš¨ìœ¨ì ì¸ ë‹¤ìš´ìƒ˜í”Œë§ - HWNet ì •ì²´ì„± ìœ ì§€"""
    def __init__(self, nIn, nOut):
        super().__init__()
        self.nIn, self.nOut = nIn, nOut
        nConv = nOut - nIn if self.nIn < self.nOut else nOut
        self.conv3x3 = DWSConv(nIn, nConv, kSize=3, stride=2, padding=1, bn_acti=False)
        self.max_pool = nn.MaxPool2d(2, stride=2, padding=0)
        self.bn = nn.BatchNorm2d(nOut, eps=1e-3)
        self.acti = nn.SiLU(inplace=True)

    def forward(self, input):
        output = self.conv3x3(input)
        if self.nIn < self.nOut:
            output = torch.cat([output, self.max_pool(input)], 1)
        return self.acti(self.bn(output))

def Split(x, p):
    """ì±„ë„ ë¶„í•  - HWNet/LCNetì˜ í•µì‹¬ ì •ì²´ì„±"""
    c = int(x.size(1))
    c1 = round(c * (1 - p))
    return x[:, :c1, :, :].contiguous(), x[:, c1:, :, :].contiguous()

class TCA(nn.Module):
    """Triple Context Aggregation - ë‹¤ì¤‘ ìŠ¤ì¼€ì¼ íŠ¹ì§• ì¶”ì¶œ"""
    def __init__(self, c, d=1, kSize=3):
        super().__init__()
        self.conv3x3 = DWSConv(c, c, kSize, 1, padding=1, bn_acti=True)
        self.dconv3x3 = Conv(c, c, (kSize, kSize), 1, padding=(1, 1), groups=c, bn_acti=True)
        self.ddconv3x3 = Conv(c, c, (kSize, kSize), 1, padding=(d, d), groups=c, dilation=(d, d), bn_acti=True)
        self.bn = nn.BatchNorm2d(c, eps=1e-3)
        self.acti = nn.SiLU(inplace=True)

    def forward(self, input):
        br = self.conv3x3(input)
        br1 = self.dconv3x3(br)
        br2 = self.ddconv3x3(br)
        br = br + br1 + br2
        return self.acti(self.bn(br))

class PCT(nn.Module):
    """Partial Channel Transformation - HWNetì˜ í•µì‹¬ ë³‘ëª© êµ¬ì¡°"""
    def __init__(self, nIn, d=1, p=0.25):
        super().__init__()
        self.p = p
        c = nIn - round(nIn * (1 - p))
        self.TCA = TCA(c, d)
        self.conv1x1 = Conv(nIn, nIn, 1, 1, padding=0, bn_acti=True)

    def forward(self, input):
        x1, x2 = Split(input, self.p)
        x2 = self.TCA(x2)
        output = torch.cat([x1, x2], dim=1)
        return self.conv1x1(output) + input

class MorphGradientFocus(nn.Module):
    """ëª¨í´ë¡œì§€ ê¸°ë°˜ ì—£ì§€ ê°•í™” - íŒŒë¼ë¯¸í„° íš¨ìœ¨ì """
    def __init__(self, in_channels, k=3):
        super().__init__()
        self.pad  = k // 2
        self.fuse = Conv(in_channels + 1, in_channels, 1, 1, padding=0, bn_acti=True)

    def forward(self, x):
        intensity = x.mean(dim=1, keepdim=True)
        dilated = F.max_pool2d(intensity, 3, stride=1, padding=self.pad)
        eroded  = -F.max_pool2d(-intensity, 3, stride=1, padding=self.pad)
        return self.fuse(torch.cat([x, dilated - eroded], dim=1))

# --- v7_lite ê²½ëŸ‰ ëª¨ë“ˆë“¤ ---

class LiteAdaptiveASPP(nn.Module):
    """ê²½ëŸ‰í™”ëœ Adaptive ASPP - íŒŒë¼ë¯¸í„° ìµœì†Œí™”"""
    def __init__(self, in_channels, out_channels, num_classes):
        super().__init__()
        self.num_classes = num_classes
        
        # ê²½ëŸ‰í™”: ëª¨ë“  ê²½ìš°ì— ë™ì¼í•œ êµ¬ì¡° ì‚¬ìš© (ì°¨ì´ëŠ” ìµœì†Œí™”)
        mid_channels = max(3, out_channels // 5)  # ë” ì‘ì€ ì±„ë„
        
        # Multi-classì¼ ë•Œë§Œ dilation 1ê°œ ì¶”ê°€
        if num_classes > 2:
            dilations = [1, 6, 12]  # 3ê°œë¡œ ì¤„ì„
        else:
            dilations = [1, 6]      # 2ê°œë§Œ
        
        self.branches = nn.ModuleList()
        for d in dilations:
            if d == 1:
                self.branches.append(DWSConv(in_channels, mid_channels, 1, 1, padding=0, bn_acti=True))
            else:
                self.branches.append(DWSConv(in_channels, mid_channels, 3, 1, padding=d, dilation=(d, d), bn_acti=True))
        
        # Global pooling ê²½ëŸ‰í™”
        self.global_pool = nn.Sequential(
            nn.AdaptiveAvgPool2d(1),
            nn.Conv2d(in_channels, mid_channels, 1, bias=False),
            nn.Sigmoid()  # BN + activation ì œê±°
        )
        
        # Fusion ê²½ëŸ‰í™”
        total_channels = mid_channels * (len(dilations) + 1)
        self.fusion = nn.Conv2d(total_channels, out_channels, 1, bias=False)  # DWSConv â†’ Convë¡œ ë³€ê²½
        self.bn = nn.BatchNorm2d(out_channels, eps=1e-3)
        self.acti = nn.SiLU(inplace=True)
        
    def forward(self, x):
        outputs = []
        
        # Multi-scale features
        for branch in self.branches:
            outputs.append(branch(x))
        
        # Global context
        gp = self.global_pool(x)
        gp = F.interpolate(gp, size=x.shape[2:], mode='bilinear', align_corners=True)
        outputs.append(gp)
        
        # Concatenate and fuse
        concat = torch.cat(outputs, dim=1)
        out = self.fusion(concat)
        return self.acti(self.bn(out))

class LiteClassAttention(nn.Module):
    """ê²½ëŸ‰í™”ëœ Class-Aware Attention"""
    def __init__(self, channels, num_classes, reduction=24):  # reduction ì¦ê°€
        super().__init__()
        self.num_classes = num_classes
        
        # Multi-classì—¬ë„ ì±„ë„ ì–´í…ì…˜ë§Œ ì‚¬ìš© (ê³µê°„ ì–´í…ì…˜ ì œê±°)
        self.channel_attention = nn.Sequential(
            nn.AdaptiveAvgPool2d(1),
            nn.Conv2d(channels, max(1, channels // reduction), 1, bias=False),
            nn.ReLU(inplace=True),
            nn.Conv2d(max(1, channels // reduction), channels, 1, bias=False),
            nn.Sigmoid()
        )
        
    def forward(self, x):
        # Channel attentionë§Œ ì ìš© (ê³µê°„ ì–´í…ì…˜ ì™„ì „ ì œê±°)
        ca = self.channel_attention(x)
        return x * ca

class LitePyramidDecoder(nn.Module):
    """ê²½ëŸ‰í™”ëœ Pyramid Decoder"""
    def __init__(self, high_channels, low_channels, out_channels, num_classes):
        super().__init__()
        self.num_classes = num_classes
        
        # Lateral connection ê²½ëŸ‰í™”
        self.lateral = nn.Conv2d(low_channels, out_channels, 1, bias=False)  # Convë¡œ ë‹¨ìˆœí™”
        self.lateral_bn = nn.BatchNorm2d(out_channels, eps=1e-3)
        
        # Top-down pathway
        self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)
        
        # Feature refinement ë‹¨ìˆœí™” (ëª¨ë“  ê²½ìš°ì— 1-layerë§Œ)
        self.refine = DWSConv(high_channels + out_channels, out_channels, 3, 1, padding=1, bn_acti=True)
        
    def forward(self, high_feat, low_feat):
        # Upsample high-level features
        high_up = self.up(high_feat)
        
        # Process low-level features (ê²½ëŸ‰í™”)
        low_lateral = F.relu(self.lateral_bn(self.lateral(low_feat)), inplace=True)
        
        # Combine features
        combined = torch.cat([high_up, low_lateral], dim=1)
        
        # Refine combined features
        return self.refine(combined)

# --- ìµœì¢… ì œì¶œ ëª¨ë¸: HWNetUltra_v7_lite (ê·¹ë„ ê²½ëŸ‰í™”) ---
class submission_HWNetUltra_v7_lite(nn.Module):
    def __init__(self, in_channels=3, num_classes=1):
        super().__init__()
        # v5 ê¸°ë°˜ ì„¤ì • ìœ ì§€
        block_1 = 2
        block_2 = 2
        C = 8
        P = 0.25
        dilation_block_1 = [2, 3]
        dilation_block_2 = [2, 4]

        # ì—£ì§€ ê°•í™” ëª¨ë“ˆ (v5ì™€ ë™ì¼)
        self.edge_focus = MorphGradientFocus(in_channels)
        
        # ì´ˆê¸° íŠ¹ì§• ì¶”ì¶œ (v5ì™€ ë™ì¼)
        self.Init_Block = nn.Sequential(
            DWSConv(in_channels, C, 3, 2, padding=1, bn_acti=True),
            DWSConv(C, C, 3, 1, padding=1, bn_acti=True)
        )

        # 1ë‹¨ê³„ ì¸ì½”ë” (v5ì™€ ë™ì¼)
        self.LC_Block_1 = nn.Sequential()
        self.LC_Block_1.add_module("downsample", DownSamplingBlock(C, C * 2))
        for i in range(block_1):
            self.LC_Block_1.add_module(f"LC_Module_1_{i}", PCT(C * 2, d=dilation_block_1[i], p=P))

        # 2ë‹¨ê³„ ì¸ì½”ë” (v5ì™€ ë™ì¼)
        self.LC_Block_2 = nn.Sequential()
        self.LC_Block_2.add_module("downsample", DownSamplingBlock(C * 2, C * 4))
        for i in range(block_2):
            self.LC_Block_2.add_module(f"LC_Module_2_{i}", PCT(C * 4, d=dilation_block_2[i], p=P))
        
        # ğŸ†• v7_lite: ê²½ëŸ‰í™”ëœ Adaptive ASPP
        self.aspp = LiteAdaptiveASPP(C * 4, C * 4, num_classes)
        
        # ğŸ†• v7_lite: ê²½ëŸ‰í™”ëœ Class-Aware Attention
        self.attention = LiteClassAttention(C * 4, num_classes, reduction=24)
        
        # ğŸ†• v7_lite: ê²½ëŸ‰í™”ëœ Pyramid Decoder
        self.dec2 = LitePyramidDecoder(C * 4, C * 2, C * 2, num_classes)
        self.dec1 = LitePyramidDecoder(C * 2, C, C, num_classes)
        
        # ìµœì¢… ì¶œë ¥ ê²½ëŸ‰í™” (ë™ì¼í•œ êµ¬ì¡°)
        self.final_up = nn.Sequential(
            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True),
            DWSConv(C, C, 3, 1, padding=1, bn_acti=True),
            DWSConv(C, C//2, 3, 1, padding=1, bn_acti=True),
            nn.Conv2d(C//2, num_classes, kernel_size=1)
        )

    def forward(self, input):
        # ì—£ì§€ ê°•í™”
        x_init = self.edge_focus(input)
        
        # ì¸ì½”ë”
        x0 = self.Init_Block(x_init)
        x1 = self.LC_Block_1(x0)
        x2 = self.LC_Block_2(x1)
        
        # ğŸ†• v7_lite: ê²½ëŸ‰í™”ëœ multi-scale context
        x2 = self.aspp(x2)
        
        # ğŸ†• v7_lite: ê²½ëŸ‰í™”ëœ class-aware attention
        x2 = self.attention(x2)
        
        # ğŸ†• v7_lite: ê²½ëŸ‰í™”ëœ pyramid decoder
        d2 = self.dec2(x2, x1)
        d1 = self.dec1(d2, x0)
        
        # ìµœì¢… ì¶œë ¥
        return self.final_up(d1)

if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸ ë° íŒŒë¼ë¯¸í„° ê²€ì¦
    for num_classes in [2, 21]:
        net = submission_HWNetUltra_v7_lite(in_channels=3, num_classes=num_classes)
        p = sum(p.numel() for p in net.parameters() if p.requires_grad)
        print(f"Classes: {num_classes}, Params: {p:,}")
        
        # íŒŒë¼ë¯¸í„° ëª©í‘œ ê²€ì¦
        if p < 10000:
            print(f"âœ… ëª©í‘œ ë‹¬ì„±: {p}/10,000 ({10000-p} ì—¬ìœ )")
        elif p <= 17000:
            print(f"âš ï¸ í—ˆìš© ë²”ìœ„: {p}/17,000 ({17000-p} ì—¬ìœ )")
        else:
            print(f"âŒ ì´ˆê³¼: {p}/17,000 ({p-17000} ì´ˆê³¼)")

        try:
            net.eval()
            x = torch.randn(1, 3, 256, 256)
            y = net(x)
            print(f"Input: {x.shape} â†’ Output: {y.shape}")
            assert y.shape == (1, num_classes, 256, 256)
            print("âœ… í…ŒìŠ¤íŠ¸ í†µê³¼\n")
        except Exception as e:
            print(f"âŒ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}\n")
    
    print(f"ğŸ“Š HWNetUltra_v7_lite ê²½ëŸ‰í™” ê°œì„ :")
    print(f"  ğŸ’¡ LiteAdaptiveASPP: ì±„ë„ ìµœì†Œí™” + dilation ì°¨ë³„í™”")
    print(f"  ğŸ’¡ LiteClassAttention: Channel attentionë§Œ (spatial ì œê±°)")  
    print(f"  ğŸ’¡ LitePyramidDecoder: ë‹¨ìˆœí™”ëœ feature fusion")
    print(f"  ğŸ’¡ Unified Final Layers: ëª¨ë“  í´ë˜ìŠ¤ì— ë™ì¼ êµ¬ì¡°")
    print(f"  ğŸ¯ ëª©í‘œ: 1ë§Œê°œ ë¯¸ë§Œ íŒŒë¼ë¯¸í„°ë¡œ IoU 0.43+ ë‹¬ì„±") 