#!/usr/bin/env python3
"""
Optuna ê¸°ë°˜ í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” ì‹¤í—˜ ìŠ¤í¬ë¦½íŠ¸
competition_modified.ipynbì˜ êµ¬ì¡°ë¥¼ ê·¸ëŒ€ë¡œ ëª¨ë°©í•˜ì—¬ py íŒŒì¼ë¡œ êµ¬í˜„
"""

import os
import sys
import time
import importlib
from datetime import datetime
import pandas as pd
import numpy as np
import glob
import natsort
import torch
import optuna
from optuna.integration import TensorBoardCallback
from optuna.pruners import MedianPruner, HyperbandPruner
from optuna.samplers import TPESampler, RandomSampler, CmaEsSampler
import logging

# ê¸°ì¡´ ëª¨ë“ˆ import
from competition_utils import *
from training_args import *

# Optuna ì„¤ì • import
from optuna_config import (
    EXPERIMENT_CONFIG, STUDY_CONFIG, HYPERPARAMETER_SPACE,
    EARLY_STOPPING_CONFIG, LOGGING_CONFIG, DATASET_CONFIG,
    PERFORMANCE_BENCHMARKS, validate_config
)

# Optuna ë¡œê¹… ì„¤ì •
optuna.logging.get_logger("optuna").addHandler(logging.StreamHandler(sys.stdout))

def get_dataset_threshold(dataset_name, dataset_index):
    """
    ë°ì´í„°ì…‹ë³„ IoU ì„ê³„ì¹˜ë¥¼ ë°˜í™˜í•˜ëŠ” í•¨ìˆ˜
    
    Args:
        dataset_name: ë°ì´í„°ì…‹ ì´ë¦„ (VOC, ETIS, CVPPP, CFD, CarDD)
        dataset_index: ë°ì´í„°ì…‹ ìˆœì„œ (0~4)
    
    Returns:
        float: í•´ë‹¹ ë°ì´í„°ì…‹ì˜ IoU ì„ê³„ì¹˜
    """
    base_thresholds = EARLY_STOPPING_CONFIG['dataset_thresholds']
    threshold_multiplier = EARLY_STOPPING_CONFIG['threshold_multiplier']
    
    # ê¸°ë³¸ ì„ê³„ì¹˜ ê°€ì ¸ì˜¤ê¸°
    base_threshold = base_thresholds.get(dataset_name, 0.1)  # ê¸°ë³¸ê°’ 0.1
    
    # Progressive thresholds ì˜µì…˜ ì ìš©
    if EARLY_STOPPING_CONFIG['progressive_thresholds']:
        # ë°ì´í„°ì…‹ ìˆœì„œì— ë”°ë¼ ì„ê³„ì¹˜ë¥¼ ì ì§„ì ìœ¼ë¡œ ì¦ê°€
        # ì²« ë²ˆì§¸ ë°ì´í„°ì…‹: ê¸°ë³¸ê°’ì˜ 80%
        # ë§ˆì§€ë§‰ ë°ì´í„°ì…‹: ê¸°ë³¸ê°’ì˜ 120%
        progress_factor = 0.8 + (0.4 * dataset_index / 4)  # 0.8 -> 1.2
        base_threshold *= progress_factor
    
    # ì „ì—­ multiplier ì ìš©
    final_threshold = base_threshold * threshold_multiplier
    
    return final_threshold


def log_pruning_statistics(trial, dataset_name, current_iou, threshold, pruned=False):
    """Pruning í†µê³„ë¥¼ ë¡œê¹…í•˜ëŠ” í•¨ìˆ˜"""
    status = "PRUNED" if pruned else "PASSED"
    print(f"ğŸ“Š Trial {trial.number} | {dataset_name} | IoU: {current_iou:.4f} | ì„ê³„ì¹˜: {threshold:.4f} | ìƒíƒœ: {status}")


class OptunaTrainingArgs:
    """Optuna trialì„ ê¸°ë°˜ìœ¼ë¡œ í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¥¼ ë™ì ìœ¼ë¡œ ìƒì„±í•˜ëŠ” í´ë˜ìŠ¤"""
    
    def __init__(self, trial):
        self.trial = trial
    
    def make_optimizer(self, model):
        """trialì„ ê¸°ë°˜ìœ¼ë¡œ optimizer ìƒì„± (config íŒŒì¼ ì„¤ì • ì‚¬ìš©)"""
        # configì—ì„œ optimizer choices ê°€ì ¸ì˜¤ê¸°
        optimizer_config = HYPERPARAMETER_SPACE['optimizer']
        optimizer_name = self.trial.suggest_categorical('optimizer', optimizer_config['choices'])
        
        # configì—ì„œ learning rate ë²”ìœ„ ê°€ì ¸ì˜¤ê¸°
        lr_config = HYPERPARAMETER_SPACE['learning_rate']
        lr = self.trial.suggest_float('learning_rate', lr_config['low'], lr_config['high'], 
                                     log=lr_config.get('log', False))
        
        # configì—ì„œ weight decay ë²”ìœ„ ê°€ì ¸ì˜¤ê¸°
        wd_config = HYPERPARAMETER_SPACE['weight_decay']
        weight_decay = self.trial.suggest_float('weight_decay', wd_config['low'], wd_config['high'],
                                               log=wd_config.get('log', False))
        
        if optimizer_name == "adam":
            optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)
        elif optimizer_name == "sgd":
            # SGDì¸ ê²½ìš°ì—ë§Œ momentum ì„¤ì •
            if 'momentum' in HYPERPARAMETER_SPACE:
                momentum_config = HYPERPARAMETER_SPACE['momentum']
                momentum = self.trial.suggest_float('momentum', momentum_config['low'], momentum_config['high'])
            else:
                momentum = 0.9  # ê¸°ë³¸ê°’
            optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=momentum, weight_decay=weight_decay)
        elif optimizer_name == "adamw":
            optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)
        else:
            raise ValueError(f"Unsupported optimizer: {optimizer_name}")
        
        return optimizer
    
    def make_lr_scheduler(self, optimizer):
        """trialì„ ê¸°ë°˜ìœ¼ë¡œ learning rate scheduler ìƒì„± (config íŒŒì¼ ì„¤ì • ì‚¬ìš©)"""
        scheduler_config = HYPERPARAMETER_SPACE['scheduler']
        scheduler_name = self.trial.suggest_categorical('scheduler', scheduler_config['choices'])
        
        if scheduler_name == "warmup_cosine":
            # configì—ì„œ warmup cosine ì„¤ì • ê°€ì ¸ì˜¤ê¸°
            warmup_epochs_config = HYPERPARAMETER_SPACE.get('warmup_epochs', {'low': 0, 'high': 10})
            warmup_epochs = self.trial.suggest_int('warmup_epochs', 
                                                  warmup_epochs_config['low'], warmup_epochs_config['high'])
            
            warmup_factor_config = HYPERPARAMETER_SPACE.get('warmup_factor', {'low': 0.05, 'high': 0.3})
            warmup_factor = self.trial.suggest_float('warmup_factor', 
                                                    warmup_factor_config['low'], warmup_factor_config['high'])
            
            eta_min_config = HYPERPARAMETER_SPACE.get('eta_min', {'low': 1e-7, 'high': 1e-5, 'log': True})
            eta_min = self.trial.suggest_float('eta_min', eta_min_config['low'], eta_min_config['high'],
                                              log=eta_min_config.get('log', False))
            
            lr_scheduler = WarmupCosineLR(
                optimizer,
                T_max=50,
                warmup_iters=warmup_epochs,
                warmup_factor=warmup_factor,
                eta_min=eta_min
            )
        elif scheduler_name == "warmup_poly":
            warmup_epochs_config = HYPERPARAMETER_SPACE.get('warmup_epochs_poly', {'low': 0, 'high': 8})
            warmup_epochs = self.trial.suggest_int('warmup_epochs_poly', 
                                                  warmup_epochs_config['low'], warmup_epochs_config['high'])
            
            power_config = HYPERPARAMETER_SPACE.get('poly_power', {'low': 0.7, 'high': 1.0})
            power = self.trial.suggest_float('poly_power', power_config['low'], power_config['high'])
            
            lr_scheduler = WarmupPolyLR(
                optimizer,
                T_max=DATASET_CONFIG['epochs'],
                warmup_epochs=warmup_epochs,
                power=power,
                eta_min=1e-6
            )
        elif scheduler_name == "constant":
            lr_scheduler = torch.optim.lr_scheduler.ConstantLR(optimizer, factor=1.0, total_iters=1)
        else:
            raise ValueError(f"Unsupported scheduler: {scheduler_name}")
        
        return lr_scheduler
    
    def make_loss_function(self, number_of_classes):
        """trialì„ ê¸°ë°˜ìœ¼ë¡œ loss function ìƒì„± (config íŒŒì¼ ì„¤ì • ì‚¬ìš©)"""
        BINARY_SEG = True if number_of_classes == 2 else False
        
        if BINARY_SEG and 'dice_weight' in HYPERPARAMETER_SPACE:
            dice_weight_config = HYPERPARAMETER_SPACE['dice_weight']
            dice_weight = self.trial.suggest_float('dice_weight', 
                                                  dice_weight_config['low'], dice_weight_config['high'])
            loss = DiceCELoss(weight=dice_weight, mode='binary')
        elif BINARY_SEG:
            # configì— dice_weightê°€ ì—†ìœ¼ë©´ ê¸°ë³¸ê°’ ì‚¬ìš©
            loss = DiceCELoss(weight=0.5, mode='binary')
        else:
            # multiclassì˜ ê²½ìš° ê¸°ì¡´ ì„¤ì • ìœ ì§€
            loss = UniformCBCE_LovaszProb(number_of_classes)
        
        return loss


def run_single_experiment(student_id=None, model_name=None, trial=None, save_results=False):
    """
    ë‹¨ì¼ ì‹¤í—˜ ì‹¤í–‰ í•¨ìˆ˜ (competition_modified.ipynbì˜ ë©”ì¸ ë¡œì§)
    
    Args:
        student_id: í•™ìƒ ID (Noneì´ë©´ configì—ì„œ ê°€ì ¸ì˜´)
        model_name: ëª¨ë¸ ì´ë¦„ (Noneì´ë©´ configì—ì„œ ê°€ì ¸ì˜´)
        trial: optuna trial ê°ì²´ (Noneì´ë©´ ê¸°ë³¸ ì„¤ì • ì‚¬ìš©)
        save_results: ê²°ê³¼ ì €ì¥ ì—¬ë¶€
    
    Returns:
        mean_iou: 5ê°œ ë°ì´í„°ì…‹ì˜ í‰ê·  IoU
        results_df: ì‹¤í—˜ ê²°ê³¼ DataFrame
    """
    
    # ì‹¤í—˜ ì‹œì‘ ì‹œê°„
    start_time = time.time()
    now = datetime.now()
    experiments_time = now.strftime("%y%m%d_%H%M%S")
    
    if trial is not None:
        experiments_time += f"_trial{trial.number}"
    
    print(f'=== Experiment Start Time: {experiments_time} ===')
    
    # ì„¤ì •ì—ì„œ ê¸°ë³¸ê°’ ê°€ì ¸ì˜¤ê¸°
    if student_id is None:
        student_id = EXPERIMENT_CONFIG['student_id']
    if model_name is None:
        model_name = EXPERIMENT_CONFIG['model_name']
    
    # ê¸°ë³¸ ì„¤ì •
    model_name_full = f'submission_{model_name}'
    module_path = f"models.submission_{student_id}.{model_name_full}"
    module = importlib.import_module(module_path)
    model_class = getattr(module, model_name_full)
    
    # ë°ì´í„°ì…‹ ì„¤ì • (configì—ì„œ ê°€ì ¸ì˜¤ê¸°)
    Dataset_root = DATASET_CONFIG['dataset_root']
    Dataset_Name_list = DATASET_CONFIG['dataset_names']
    number_of_classes_dict = DATASET_CONFIG['num_classes']
    
    # ì‹¤í—˜ ì„¤ì • (configì—ì„œ ê°€ì ¸ì˜¤ê¸°)
    epochs = DATASET_CONFIG['epochs']
    EARLY_STOP = DATASET_CONFIG['early_stop_patience']
    batch_size = DATASET_CONFIG['batch_size']
    EXCLUDE_BACKGROUND = DATASET_CONFIG['exclude_background']
    THRESHOLD = DATASET_CONFIG['threshold']
    device = "cuda" if torch.cuda.is_available() else "cpu"
    
    # ê²°ê³¼ ì €ì¥ ì„¤ì •
    output_root = 'output'
    output_root = f'{output_root}/output_{experiments_time}'
    os.makedirs(output_root, exist_ok=True)
    
    # vis_root ì„¤ì • ìˆ˜ì •: save_resultsê°€ Falseì—¬ë„ ì„ì‹œ ë””ë ‰í† ë¦¬ ìƒì„±
    if save_results:
        vis_root = f'vis/OPTUNA_OUTPUTS_{experiments_time}'
        os.makedirs(vis_root, exist_ok=True)
    else:
        # save_resultsê°€ Falseì—¬ë„ ì„ì‹œ ë””ë ‰í† ë¦¬ ìƒì„± (test í•¨ìˆ˜ ì—ëŸ¬ ë°©ì§€)
        vis_root = f'temp_vis_{experiments_time}'
        os.makedirs(vis_root, exist_ok=True)
    
    # ê²°ê³¼ DataFrame ì´ˆê¸°í™”
    eval_columns = ['Experiment Time', 'Train Time', 'Dataset Name', 'Model Name', 
                   'Val Loss', 'Test Loss', 'IoU', 'Dice', 'Precision', 'Recall', 
                   'Total Params', 'Train-Prediction Time']
    df = pd.DataFrame(index=None, columns=eval_columns)
    
    # Optuna training args ì´ˆê¸°í™” (trialì´ ìˆëŠ” ê²½ìš°)
    optuna_args = OptunaTrainingArgs(trial) if trial is not None else None
    
    iou_scores = []
    seed = 1
    
    # ê° ë°ì´í„°ì…‹ì— ëŒ€í•´ ì‹¤í—˜ ìˆ˜í–‰
    for j, Dataset_Name in enumerate(Dataset_Name_list):
        print(f'\n=== Dataset: {Dataset_Name} ({j+1}/{len(Dataset_Name_list)}) ===')
        control_random_seed(seed)
        
        # ë°ì´í„°ì…‹ ê²½ë¡œ ì„¤ì •
        Dataset_dir = Dataset_root + '/' + Dataset_Name
        Height, Width = (256, 256)
        in_channels = 3
        number_of_classes = number_of_classes_dict[Dataset_Name]
        BINARY_SEG = True if number_of_classes == 2 else False
        exclude_background = EXCLUDE_BACKGROUND
        out_channels = 1 if BINARY_SEG else number_of_classes
        
        # ë°ì´í„° ë¡œë“œ
        train_image_path_list = natsort.natsorted(glob.glob(f"{Dataset_dir}/train/Originals/*"))
        train_target_path_list = natsort.natsorted(glob.glob(f"{Dataset_dir}/train/Masks/*"))
        validation_image_path_list = natsort.natsorted(glob.glob(f"{Dataset_dir}/val/Originals/*"))
        validation_target_path_list = natsort.natsorted(glob.glob(f"{Dataset_dir}/val/Masks/*"))
        test_image_path_list = natsort.natsorted(glob.glob(f"{Dataset_dir}/test/Originals/*"))
        test_target_path_list = natsort.natsorted(glob.glob(f"{Dataset_dir}/test/Masks/*"))
        
        print(f'train/val/test: {len(train_image_path_list)}/{len(validation_image_path_list)}/{len(test_image_path_list)}')
        
        # ë°ì´í„°ì…‹ ë° ë°ì´í„°ë¡œë” ìƒì„±
        train_dataset = ImagesDataset(train_image_path_list, train_target_path_list)
        validation_dataset = ImagesDataset(validation_image_path_list, validation_target_path_list)
        test_dataset = ImagesDataset(test_image_path_list, test_target_path_list)
        
        train_loader = SegDataLoader(train_dataset, batch_size=batch_size, num_workers=4, 
                                   pin_memory=True, shuffle=True, drop_last=True, fill_last_batch=False)
        validation_loader = SegDataLoader(validation_dataset, batch_size=batch_size, num_workers=4, pin_memory=True)
        test_loader = SegDataLoader(test_dataset, batch_size=batch_size, num_workers=4, pin_memory=True)
        
        # ëª¨ë¸ ìƒì„±
        print(f'{model_name_full} Dataset: {Dataset_Name}) ({j+1}/{len(Dataset_Name_list)})')
        output_dir = output_root + f'/{model_name_full}_{Dataset_Name}'
        control_random_seed(seed)
        
        model = model_class(in_channels, out_channels)
        model = model.to(device)
        
        # í•˜ì´í¼íŒŒë¼ë¯¸í„° ì„¤ì • (Optuna trial ë˜ëŠ” ê¸°ë³¸ê°’)
        if optuna_args is not None:
            optimizer = optuna_args.make_optimizer(model)
            lr_scheduler = optuna_args.make_lr_scheduler(optimizer)
            criterion = optuna_args.make_loss_function(number_of_classes)
        else:
            # ê¸°ë³¸ ì„¤ì • ì‚¬ìš©
            optimizer = Make_Optimizer(model)
            lr_scheduler = Make_LR_Scheduler(optimizer)
            criterion = Make_Loss_Function(number_of_classes)
        
        # ì‹œê°í™” ë””ë ‰í† ë¦¬ ì„¤ì •
        vis_dataset_root = f"{vis_root}/{Dataset_Name}"
        os.makedirs(vis_dataset_root, exist_ok=True)
        
        # ì‹¤í—˜ ì‹¤í–‰
        df = Execute_Experiment(
            model_name_full, model, Dataset_Name, train_loader, validation_loader, test_loader,
            optimizer, lr_scheduler, criterion, number_of_classes, df, epochs, device, output_dir,
            BINARY_SEG, exclude_background, out_channels, seed, THRESHOLD, EARLY_STOP, 
            save_results, vis_dataset_root, experiments_time
        )
        
        # save_resultsê°€ Falseì¸ ê²½ìš° ì„ì‹œ ì‹œê°í™” íŒŒì¼ë“¤ ì •ë¦¬
        if not save_results:
            import shutil
            if os.path.exists(vis_dataset_root):
                shutil.rmtree(vis_dataset_root)
        
        # IoU ì ìˆ˜ ì¶”ì¶œ
        current_iou = df.iloc[-1]['IoU']  # ë§ˆì§€ë§‰ìœ¼ë¡œ ì¶”ê°€ëœ í–‰ì˜ IoU
        iou_scores.append(current_iou)
        
        print(f'Dataset {Dataset_Name} IoU: {current_iou:.4f}')
        
        # ğŸ¯ ê° ë°ì´í„°ì…‹ ì™„ë£Œ í›„ ì¤‘ê°„ í‰ê°€ ë° pruning
        if trial is not None:
            # í˜„ì¬ê¹Œì§€ì˜ í‰ê·  IoUë¡œ ì¤‘ê°„ ë³´ê³ 
            current_mean_iou = np.mean(iou_scores)
            trial.report(current_mean_iou, j)
            
            # ë°ì´í„°ì…‹ë³„ ì¤‘ê°„ pruning ì²´í¬ (config ì„¤ì •ì— ë”°ë¼)
            if EARLY_STOPPING_CONFIG['intermediate_pruning']:
                dataset_threshold = get_dataset_threshold(Dataset_Name, j)
                
                print(f'ğŸ“Š Dataset {Dataset_Name} í‰ê°€: IoU {current_iou:.4f} (ì„ê³„ì¹˜: {dataset_threshold:.4f})')
                
                # í˜„ì¬ ë°ì´í„°ì…‹ì˜ IoUê°€ ì„ê³„ì¹˜ ë¯¸ë‹¬ì¸ì§€ ì²´í¬
                if current_iou < dataset_threshold:
                    print(f'âš ï¸  Dataset {Dataset_Name} IoU {current_iou:.4f} < ì„ê³„ì¹˜ {dataset_threshold:.4f}')
                    
                    # Optuna prunerì˜ íŒë‹¨ë„ ê³ ë ¤
                    if trial.should_prune():
                        print(f'ğŸš« Trial {trial.number} pruned after {Dataset_Name} (IoU: {current_iou:.4f} < {dataset_threshold:.4f})')
                        if not save_results and vis_root.startswith('temp_vis_'):
                            import shutil
                            if os.path.exists(vis_root):
                                shutil.rmtree(vis_root)
                        raise optuna.TrialPruned()
                    else:
                        print(f'ğŸ”„ ì„ê³„ì¹˜ ë¯¸ë‹¬ì´ì§€ë§Œ pruner íŒë‹¨ìœ¼ë¡œ ê³„ì† ì§„í–‰ (Trial {trial.number})')
                else:
                    print(f'âœ… Dataset {Dataset_Name} ì„ê³„ì¹˜ í†µê³¼: {current_iou:.4f} >= {dataset_threshold:.4f}')
            
            # ì§„í–‰ë¥  í‘œì‹œ
            progress = ((j + 1) / len(Dataset_Name_list)) * 100
            print(f'ğŸ“ˆ ì§„í–‰ë¥ : {progress:.1f}% ({j+1}/{len(Dataset_Name_list)} ë°ì´í„°ì…‹ ì™„ë£Œ)')
            print(f'ğŸ”¢ í˜„ì¬ê¹Œì§€ í‰ê·  IoU: {current_mean_iou:.4f}')
            print('-' * 60)
    
    # í‰ê·  IoU ê³„ì‚°
    mean_iou = np.mean(iou_scores)
    print(f'\n=== Final Mean IoU: {mean_iou:.4f} ===')
    
    # ğŸ¯ 5ê°œ ë°ì´í„°ì…‹ ì™„ë£Œ í›„ ìµœì¢… pruning ì²´í¬ (ì„ íƒì )
    if (trial is not None and EARLY_STOPPING_CONFIG['enabled'] and 
        mean_iou < EARLY_STOPPING_CONFIG.get('final_threshold', 0.05)):
        print(f"Trial {trial.number} completed but final Mean IoU too low: {mean_iou:.4f}")
        # ì—¬ê¸°ì„œëŠ” TrialPrunedë¥¼ ë°œìƒì‹œí‚¤ì§€ ì•Šê³ , ë‹¨ìˆœíˆ ë‚®ì€ ì ìˆ˜ ë°˜í™˜
    
    # ê²°ê³¼ ì €ì¥
    if save_results:
        df.to_csv(output_root + '/' + f'Competition_{experiments_time}.csv', 
                 index=False, header=True, encoding="cp949")
    
    # save_resultsê°€ Falseì¸ ê²½ìš° ì„ì‹œ vis_root ë””ë ‰í† ë¦¬ ì •ë¦¬
    if not save_results and vis_root.startswith('temp_vis_'):
        import shutil
        if os.path.exists(vis_root):
            shutil.rmtree(vis_root)
    
    total_time = time.time() - start_time
    print(f'Total experiment time: {total_time/60:.2f} minutes')
    
    return mean_iou, df


def objective(trial):
    """Optuna objective í•¨ìˆ˜ (config ì„¤ì • ì‚¬ìš©)"""
    try:
        # configì—ì„œ ì„¤ì • ê°€ì ¸ì˜¤ê¸°
        mean_iou, _ = run_single_experiment(trial=trial, save_results=LOGGING_CONFIG['save_intermediate_results'])
        return mean_iou
        
    except optuna.TrialPruned:
        # Pruningì€ ì •ìƒì ì¸ ìµœì í™” ê³¼ì •ì´ë¯€ë¡œ re-raise
        print(f"ğŸ”¥ Trial {trial.number} was pruned (ì •ìƒì ì¸ ì¡°ê¸° ì¢…ë£Œ)")
        raise
        
    except Exception as e:
        print(f"âŒ Trial {trial.number} failed with error: {e}")
        print(f"Error type: {type(e).__name__}")
        import traceback
        print(f"Traceback: {traceback.format_exc()}")
        # ì‹¤íŒ¨í•œ trialì— ëŒ€í•´ì„œëŠ” ë§¤ìš° ë‚®ì€ ì ìˆ˜ ë°˜í™˜
        return 0.0


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜ (config ì„¤ì • ì‚¬ìš©)"""
    print("=== Optuna í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” ì‹¤í—˜ ì‹œì‘ ===")
    
    # ì„¤ì • íŒŒì¼ ìœ íš¨ì„± ê²€ì‚¬
    validate_config()
    print("âœ… ì„¤ì • íŒŒì¼ ìœ íš¨ì„± ê²€ì‚¬ í†µê³¼")
    
    # configì—ì„œ ì„¤ì • ê°€ì ¸ì˜¤ê¸°
    print(f"ğŸ“Š ì‹¤í—˜ ì„¤ì •:")
    print(f"  - ëª¨ë¸: {EXPERIMENT_CONFIG['model_name']}")
    print(f"  - Trial ìˆ˜: {EXPERIMENT_CONFIG['n_trials']}")
    print(f"  - Timeout: {EXPERIMENT_CONFIG['timeout']}")
    print(f"  - Search space í¬ê¸°: {len(HYPERPARAMETER_SPACE)}")
    
    # ë°ì´í„°ì…‹ë³„ ì„ê³„ì¹˜ ì •ë³´ ë¯¸ë¦¬ ì¶œë ¥
    if EARLY_STOPPING_CONFIG['intermediate_pruning']:
        print(f"\nğŸ¯ ë°ì´í„°ì…‹ë³„ pruning ì„ê³„ì¹˜:")
        for i, dataset_name in enumerate(DATASET_CONFIG['dataset_names']):
            threshold = get_dataset_threshold(dataset_name, i)
            print(f"  {dataset_name}: {threshold:.4f}")
        print(f"  ì„ê³„ì¹˜ ë°°ìˆ˜: {EARLY_STOPPING_CONFIG['threshold_multiplier']}")
        print(f"  Progressive ì„ê³„ì¹˜: {EARLY_STOPPING_CONFIG['progressive_thresholds']}")
    else:
        print(f"\nâš ï¸  ì¤‘ê°„ pruning ë¹„í™œì„±í™”ë¨ - ëª¨ë“  ë°ì´í„°ì…‹ ì™„ë£Œ í›„ í‰ê°€")
    
    # Optuna study ìƒì„± (config ì„¤ì • ì‚¬ìš©)
    if STUDY_CONFIG['persistent_study']:
        # ì§€ì†ì  ìŠ¤í„°ë””: ê³ ì •ëœ ì´ë¦„ ì‚¬ìš©
        study_name = STUDY_CONFIG['study_base_name']
        storage_name = f"sqlite:///{study_name}.db" if LOGGING_CONFIG['sqlite_storage'] else None
        print(f"ğŸ”„ ì§€ì†ì  ìŠ¤í„°ë”” ëª¨ë“œ: ê¸°ì¡´ DB ì¬ì‚¬ìš©")
    else:
        # ì¼íšŒì„± ìŠ¤í„°ë””: íƒ€ì„ìŠ¤íƒ¬í”„ ì´ë¦„ ì‚¬ìš©  
        study_name = f"semantic_segmentation_optimization_{datetime.now().strftime('%y%m%d_%H%M%S')}"
        storage_name = f"sqlite:///{study_name}.db" if LOGGING_CONFIG['sqlite_storage'] else None
        print(f"ğŸ†• ìƒˆë¡œìš´ ìŠ¤í„°ë”” ëª¨ë“œ: ìƒˆ DB ìƒì„±")
    
    # Sampler ìƒì„±
    sampler_name = STUDY_CONFIG['sampler']
    sampler_kwargs = STUDY_CONFIG['sampler_kwargs']
    
    if sampler_name == 'TPESampler':
        sampler = TPESampler(**sampler_kwargs)
    elif sampler_name == 'RandomSampler':
        sampler = RandomSampler(seed=sampler_kwargs.get('seed', 42))
    elif sampler_name == 'CmaEsSampler':
        sampler = CmaEsSampler(seed=sampler_kwargs.get('seed', 42))
    else:
        sampler = TPESampler(**sampler_kwargs)
    
    # Pruner ìƒì„± (prunerë³„ ì˜¬ë°”ë¥¸ íŒŒë¼ë¯¸í„° ì‚¬ìš©)
    pruner_name = STUDY_CONFIG['pruner']
    
    if pruner_name == 'MedianPruner':
        pruner_kwargs = STUDY_CONFIG['median_pruner_kwargs']
        pruner = MedianPruner(**pruner_kwargs)
    elif pruner_name == 'HyperbandPruner':
        pruner_kwargs = STUDY_CONFIG['hyperband_pruner_kwargs']
        pruner = HyperbandPruner(**pruner_kwargs)
    else:
        # ê¸°ë³¸ê°’: MedianPruner
        pruner_kwargs = STUDY_CONFIG['median_pruner_kwargs']
        pruner = MedianPruner(**pruner_kwargs)
    
    study = optuna.create_study(
        direction=STUDY_CONFIG['direction'],
        sampler=sampler,
        pruner=pruner,
        study_name=study_name,
        storage=storage_name,
        load_if_exists=True
    )
    
    print(f"Study name: {study_name}")
    if storage_name:
        print(f"Storage: {storage_name}")
    
    # ê¸°ì¡´ ìŠ¤í„°ë”” ì •ë³´ ì¶œë ¥
    existing_trials = len(study.trials)
    if existing_trials > 0:
        print(f"ğŸ“š ê¸°ì¡´ ìŠ¤í„°ë”” ë°œê²¬: {existing_trials}ê°œ trial ì´ë¯¸ ì™„ë£Œ")
        
        # ê¸°ì¡´ ìµœê³  ì„±ëŠ¥ ì¶œë ¥
        completed_trials = [t for t in study.trials if t.state == optuna.trial.TrialState.COMPLETE]
        if completed_trials:
            print(f"ğŸ† ê¸°ì¡´ ìµœê³  Mean IoU: {study.best_value:.4f} (Trial {study.best_trial.number})")
            print(f"ğŸ”§ ê¸°ì¡´ ìµœê³  íŒŒë¼ë¯¸í„°:")
            for key, value in study.best_params.items():
                print(f"  {key}: {value}")
        
        # ìŠ¤í„°ë”” ë¦¬ì…‹ ì˜µì…˜
        if STUDY_CONFIG['reset_study_on_start']:
            print(f"âš ï¸  ìŠ¤í„°ë”” ë¦¬ì…‹ ì˜µì…˜ í™œì„±í™” - ê¸°ì¡´ ë°ì´í„° ì‚­ì œ")
            import sqlite3
            if storage_name and storage_name.startswith('sqlite:///'):
                db_path = storage_name.replace('sqlite:///', '')
                if os.path.exists(db_path):
                    os.remove(db_path)
                    print(f"ğŸ—‘ï¸  ê¸°ì¡´ DB ì‚­ì œ: {db_path}")
                    # ìƒˆë¡œìš´ ìŠ¤í„°ë”” ìƒì„±
                    study = optuna.create_study(
                        direction=STUDY_CONFIG['direction'],
                        sampler=sampler,
                        pruner=pruner,
                        study_name=study_name,
                        storage=storage_name,
                        load_if_exists=False
                    )
    else:
        print(f"ğŸ†• ìƒˆë¡œìš´ ìŠ¤í„°ë”” ì‹œì‘")
    
    # ìµœì í™” ì‹¤í–‰ (config ì„¤ì • ì‚¬ìš©)
    n_trials = EXPERIMENT_CONFIG['n_trials']
    timeout = EXPERIMENT_CONFIG['timeout']
    total_planned_trials = existing_trials + n_trials
    print(f"ğŸš€ ìµœì í™” ì‹œì‘: {n_trials}ê°œ ìƒˆ trial ì‹¤í–‰ (ì´ {total_planned_trials}ê°œ ì˜ˆìƒ)")
    
    study.optimize(objective, n_trials=n_trials, timeout=timeout)
    
    # ê²°ê³¼ ì¶œë ¥
    print("\n=== ìµœì í™” ì™„ë£Œ ===")
    total_trials = len(study.trials)
    new_trials = total_trials - existing_trials
    print(f"Total trials in study: {total_trials} (ìƒˆë¡œ ì¶”ê°€: {new_trials})")
    
    # Pruning í†µê³„ ê³„ì‚°
    completed_trials = [t for t in study.trials if t.state == optuna.trial.TrialState.COMPLETE]
    pruned_trials = [t for t in study.trials if t.state == optuna.trial.TrialState.PRUNED]
    failed_trials = [t for t in study.trials if t.state == optuna.trial.TrialState.FAIL]
    
    print(f"âœ… Completed trials: {len(completed_trials)}")
    print(f"âœ‚ï¸  Pruned trials: {len(pruned_trials)}")
    print(f"âŒ Failed trials: {len(failed_trials)}")
    
    if len(completed_trials) > 0:
        print(f"ğŸ† Best trial number: {study.best_trial.number}")
        print(f"ğŸ¯ Best value (Mean IoU): {study.best_value:.4f}")
        
        print("\nğŸ”§ Best parameters:")
        for key, value in study.best_params.items():
            print(f"  {key}: {value}")
            
        # ê°œì„  íˆìŠ¤í† ë¦¬ (ìµœê·¼ ëª‡ ê°œ trialë§Œ)
        if STUDY_CONFIG['persistent_study'] and new_trials > 0:
            print(f"\nğŸ“ˆ ì´ë²ˆ ì„¸ì…˜ ê°œì„  íˆìŠ¤í† ë¦¬:")
            recent_completed = [t for t in completed_trials if t.number >= existing_trials]
            if recent_completed:
                for trial in recent_completed[-3:]:  # ìµœê·¼ 3ê°œë§Œ
                    print(f"  Trial {trial.number}: {trial.value:.4f}")
            else:
                print(f"  ì´ë²ˆ ì„¸ì…˜ì—ì„œ ì™„ë£Œëœ trial ì—†ìŒ")
    else:
        print("âš ï¸  ëª¨ë“  trialì´ ì™„ë£Œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    
    # ë°ì´í„°ì…‹ë³„ ì„ê³„ì¹˜ ì •ë³´ ì¶œë ¥
    if EARLY_STOPPING_CONFIG['intermediate_pruning']:
        print(f"\nğŸ“Š ì‚¬ìš©ëœ ë°ì´í„°ì…‹ë³„ ì„ê³„ì¹˜:")
        for i, dataset_name in enumerate(DATASET_CONFIG['dataset_names']):
            threshold = get_dataset_threshold(dataset_name, i)
            print(f"  {dataset_name}: {threshold:.4f}")
        print(f"  ì„ê³„ì¹˜ ë°°ìˆ˜: {EARLY_STOPPING_CONFIG['threshold_multiplier']}")
    
    # ìµœì  í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¡œ ìµœì¢… ì‹¤í—˜ ì‹¤í–‰ (ê²°ê³¼ ì €ì¥)
    print("\n=== ìµœì  í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¡œ ìµœì¢… ì‹¤í—˜ ì‹¤í–‰ ===")
    
    # ìµœì  trial ê°ì²´ ìƒì„± (ê²°ê³¼ ì €ì¥ìš©)
    class BestTrial:
        def __init__(self, best_params):
            self.params = best_params
            self.number = "BEST"
        
        def suggest_categorical(self, name, choices):
            return self.params[name]
        
        def suggest_float(self, name, low, high, log=False):
            return self.params[name]
        
        def suggest_int(self, name, low, high):
            return self.params[name]
    
    best_trial = BestTrial(study.best_params)
    
    # final_mean_iou, final_df = run_single_experiment(
    #     trial=best_trial, save_results=True
    # )
    
    print(f"\n=== ì™„ë£Œ ===")
    #print(f"Final Mean IoU with best parameters: {final_mean_iou:.4f}")
    
    # ìµœì í™” íˆìŠ¤í† ë¦¬ ì €ì¥
    history_df = study.trials_dataframe()
    #history_df.to_csv(f'optuna_history_{study_name}.csv', index=False)
    #print(f"Optimization history saved to: optuna_history_{study_name}.csv")
    
    return study


if __name__ == "__main__":
    start_time = time.time()
    # GPU ë©”ëª¨ë¦¬ ì •ë¦¬
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
    
    main() 
    
    import requests

    end_time = time.time()
    elapsed_time = end_time - start_time
    hours = int(elapsed_time // 3600)
    minutes = int((elapsed_time % 3600) // 60)
    seconds = int(elapsed_time % 60)

    webhook_url = "https://discord.com/api/webhooks/1382936206731640912/FnHMkJjZo5UrE-ZXevPA9SIP-a-GRswUHb4jb4gjIsD7_McJ_ZR-h-rHm920ON-hHfIn"

    payload = {
        "content": f'optuna ìµœì í™” ì™„ë£Œ!\nì†Œìš” ì‹œê°„: {horus}ì‹œê°„ {minutes}ë¶„ {seconds}ì´ˆ'
    }

    requests.post(webhook_url, json=payload)